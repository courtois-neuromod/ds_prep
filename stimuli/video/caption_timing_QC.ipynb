{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523d703-5ce3-44f0-ac90-69df9d077a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b923ff6-c544-4a39-9324-43ccab136682",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet ipython-autotime\n",
    "!pip install moviepy==2.0.0.dev2\n",
    "!pip install imageio==2.25.1\n",
    "!pip install ImageMagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b425c31-67b2-43e6-85cf-4d6faad33e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /etc/ImageMagick-6/policy.xml | sed 's/none/read,write/g'> /etc/ImageMagick-6/policy.xml\n",
    "\n",
    "# If there is a problem with installing imagemagick please follow'\n",
    "# the instrauctions for the policy file as explained here\n",
    "# https://www.reddit.com/r/moviepy/comments/4nin6q/update_imagemagik_and_moviepy_has_broken/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1ab0d5-ff38-4d82-b081-70b6661df0a6",
   "metadata": {},
   "source": [
    "**Script takes.json files with list of words with start and end points in seconds, and creates karaoke-style captions that play simultaneously over audio to QC forced-alignment and speech-to-text ouput files**\n",
    "\n",
    "Adapted from:\n",
    "https://github.com/ramsrigouthamg/Supertranslate.ai/tree/main/Descript_like_wordhighlights_subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df04a445-14af-4b4a-a518-6ab3a7a4fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from moviepy.editor import (\n",
    "    ColorClip,\n",
    "    CompositeVideoClip,\n",
    "    TextClip,\n",
    "    VideoFileClip,\n",
    "    AudioFileClip,\n",
    ")\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5843848-b8f2-401d-9ef2-e0056480b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support functions and export parameters\n",
    "\n",
    "def split_text_into_lines(data: str) -> dict[str, str]:\n",
    "    \"\"\"Splits the text into lines.\n",
    "\n",
    "    Args:\n",
    "      data: Word extracted from the  AA aligned json file.\n",
    "\n",
    "    Returns:\n",
    "      subtitles: Subtitle transcript.\n",
    "    \"\"\"\n",
    "    max_chars = 80\n",
    "    # maximum duration in seconds\n",
    "    max_duration = 3.0\n",
    "\n",
    "    # Split if nothing is spoken (gap) for these many seconds\n",
    "    max_gap = 1.5\n",
    "    subtitles = []\n",
    "    line = []\n",
    "    line_duration = 0\n",
    "\n",
    "    for idx, word_data in enumerate(data):\n",
    "        onset = word_data[\"onset\"]\n",
    "        offset = word_data[\"offset\"]\n",
    "\n",
    "        line.append(word_data)\n",
    "        line_duration += offset - onset\n",
    "\n",
    "        temp = \" \".join(item[\"word\"] for item in line)\n",
    "\n",
    "        # Check if adding a new word exceeds the maximum character count\n",
    "        # or duration\n",
    "        new_line_chars = len(temp)\n",
    "\n",
    "        duration_exceeded = line_duration > max_duration\n",
    "        chars_exceeded = new_line_chars > max_chars\n",
    "        if idx > 0:\n",
    "            gap = word_data[\"onset\"] - data[idx - 1][\"offset\"]\n",
    "            max_gap_exceeded = gap > max_gap\n",
    "        else:\n",
    "            max_gap_exceeded = False\n",
    "\n",
    "        if duration_exceeded or chars_exceeded or max_gap_exceeded:\n",
    "            if line:\n",
    "                subtitle_line = {\n",
    "                    \"word\": \" \".join(item[\"word\"] for item in line),\n",
    "                    \"onset\": line[0][\"onset\"],\n",
    "                    \"offset\": line[-1][\"offset\"],\n",
    "                    \"textcontents\": line,\n",
    "                }\n",
    "                subtitles.append(subtitle_line)\n",
    "                line = []\n",
    "                line_duration = 0\n",
    "\n",
    "    if line:\n",
    "        subtitle_line = {\n",
    "            \"word\": \" \".join(item[\"word\"] for item in line),\n",
    "            \"onset\": line[0][\"onset\"],\n",
    "            \"offset\": line[-1][\"offset\"],\n",
    "            \"textcontents\": line,\n",
    "        }\n",
    "        subtitles.append(subtitle_line)\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "def create_caption(\n",
    "    text_json: json,\n",
    "    frame_size: tuple[int],\n",
    "    font: str = \"Helvetica-Bold\",\n",
    "    font_size: int = 80,\n",
    "    color: str = \"white\",\n",
    "    bg_color: str = \"blue\",\n",
    ") -> list:\n",
    "    \"\"\"Combines the video frames with the subtitles.\n",
    "\n",
    "    Args:\n",
    "      text_json: text extracted from the subtitles\n",
    "      frame_size: apirori defined output video frame size.\n",
    "      font: (Optional) Font for the subtitles\n",
    "      font_size: (Optional) font_size of the subtitles\n",
    "      color: (Optional) Font color of the subtitles\n",
    "      bg_color: (Optional) Background of the video frame\n",
    "\n",
    "    Return:\n",
    "      word_clips: aligned word clips\n",
    "\n",
    "    \"\"\"\n",
    "    full_duration = text_json[\"offset\"] - text_json[\"onset\"]\n",
    "\n",
    "    word_clips = []\n",
    "    xy_textclips_positions = []\n",
    "\n",
    "    x_pos = 0\n",
    "    y_pos = 0\n",
    "    frame_width = frame_size[0]\n",
    "    frame_height = frame_size[1]\n",
    "    x_buffer = frame_width * 1 / 10\n",
    "    y_buffer = frame_height * 1 / 5\n",
    "\n",
    "    space_width = \"\"\n",
    "\n",
    "    for index, word_json in enumerate(text_json[\"textcontents\"]):\n",
    "        duration = word_json[\"offset\"] - word_json[\"onset\"]\n",
    "        word_clip = (\n",
    "            TextClip(\n",
    "                word_json[\"word\"],\n",
    "                font=font,\n",
    "                fontsize=font_size,\n",
    "                color=color,\n",
    "            )\n",
    "            .set_start(text_json[\"onset\"])\n",
    "            .set_duration(full_duration)\n",
    "        )\n",
    "        word_clip_space = (\n",
    "            TextClip(\" \", font=font, fontsize=font_size, color=color)\n",
    "            .set_start(text_json[\"onset\"])\n",
    "            .set_duration(full_duration)\n",
    "        )\n",
    "        word_width, word_height = word_clip.size\n",
    "        space_width, space_height = word_clip_space.size\n",
    "        if x_pos + word_width + space_width > frame_width - 2 * x_buffer:\n",
    "            # Move to the next line\n",
    "            x_pos = 0\n",
    "            y_pos = y_pos + word_height + 40\n",
    "\n",
    "            # Store info of each word_clip created\n",
    "            xy_textclips_positions.append(\n",
    "                {\n",
    "                    \"x_pos\": x_pos + x_buffer,\n",
    "                    \"y_pos\": y_pos + y_buffer,\n",
    "                    \"width\": word_width,\n",
    "                    \"height\": word_height,\n",
    "                    \"word\": word_json[\"word\"],\n",
    "                    \"onset\": word_json[\"onset\"],\n",
    "                    \"offset\": word_json[\"offset\"],\n",
    "                    \"duration\": duration,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            word_clip = word_clip.set_position((x_pos + x_buffer, y_pos + y_buffer))\n",
    "            word_clip_space = word_clip_space.set_position(\n",
    "                (x_pos + word_width + x_buffer, y_pos + y_buffer),\n",
    "            )\n",
    "            x_pos = word_width + space_width\n",
    "        else:\n",
    "            # Store info of each word_clip created\n",
    "            xy_textclips_positions.append(\n",
    "                {\n",
    "                    \"x_pos\": x_pos + x_buffer,\n",
    "                    \"y_pos\": y_pos + y_buffer,\n",
    "                    \"width\": word_width,\n",
    "                    \"height\": word_height,\n",
    "                    \"word\": word_json[\"word\"],\n",
    "                    \"onset\": word_json[\"onset\"],\n",
    "                    \"offset\": word_json[\"offset\"],\n",
    "                    \"duration\": duration,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            word_clip = word_clip.set_position(\n",
    "                (x_pos + x_buffer, y_pos + y_buffer),\n",
    "            )\n",
    "            word_clip_space = word_clip_space.set_position(\n",
    "                (x_pos + word_width + x_buffer, y_pos + y_buffer),\n",
    "            )\n",
    "\n",
    "            x_pos = x_pos + word_width + space_width\n",
    "\n",
    "        word_clips.append(word_clip)\n",
    "        word_clips.append(word_clip_space)\n",
    "\n",
    "    for highlight_word in xy_textclips_positions:\n",
    "        word_clip_highlight = (\n",
    "            TextClip(\n",
    "                highlight_word[\"word\"],\n",
    "                font=font,\n",
    "                fontsize=font_size,\n",
    "                color=color,\n",
    "                bg_color=bg_color,\n",
    "            )\n",
    "            .set_start(highlight_word[\"onset\"])\n",
    "            .set_duration(highlight_word[\"duration\"])\n",
    "        )\n",
    "        word_clip_highlight = word_clip_highlight.set_position(\n",
    "            (highlight_word[\"x_pos\"], highlight_word[\"y_pos\"]),\n",
    "        )\n",
    "        word_clips.append(word_clip_highlight)\n",
    "\n",
    "    return word_clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33961545-0e2c-4a81-a29a-f31ffcc99f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amend the details regarding the dataset and directory paths\n",
    "stimuli_name = \"stimuli_set_name\" #e.g. narratives\n",
    "\n",
    "\n",
    "# set up your local video input path\n",
    "media_local = \"path_to_media_directory\"  # e.g. ...//narratives.stimuli/audio_files\n",
    "\n",
    "# set up your local alignment path\n",
    "align_path_aa = \"path_to_the_AssemblyAI_annotations\"  # e.g. ../transcript/word_timestamps/narratives\n",
    "\n",
    "\n",
    "# set up your local output path for the produced alignment video\n",
    "output_local = \"path_to_output_directory\"\n",
    "\n",
    "file_type = \"wav\" # chose mkv or wav\n",
    "\n",
    "frame_size = (1080, 1080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export AssemblyAI alignment as karaoke-style video\n",
    "media_dir = Path(media_local)\n",
    "\n",
    "if file_type == \"mkv\":\n",
    "    audio_files = sorted(media_dir.glob(\"*.mkv\"))\n",
    "elif file_type == \"wav\":\n",
    "    audio_files = sorted(media_dir.glob(\"*.wav\"))\n",
    "\n",
    "print(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac210523-f1bf-4c60-a225-6314095ed40d",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# iterate across the movie files\n",
    "for segment_file in audio_files:\n",
    "    segment_name = segment_file.stem\n",
    "    out_dir = Path(output_local) / stimuli_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = Path(out_dir) / f\"{segment_name}.mp4\"\n",
    "\n",
    "    j_path = Path(align_path_aa) / f\"{segment_name}.json\"\n",
    "    print(j_path)\n",
    "\n",
    "    with open(j_path) as json_file:\n",
    "        j_file = json.load(json_file)\n",
    "\n",
    "    words = j_file[\"results\"][\"channels\"][0][\"alternatives\"][0][\"words\"]\n",
    "\n",
    "    linelevel_subtitles = split_text_into_lines(words)\n",
    "\n",
    "    all_linelevel_splits = []\n",
    "\n",
    "    for line in linelevel_subtitles:\n",
    "        out = create_caption(line, frame_size)\n",
    "        all_linelevel_splits.extend(out)\n",
    "\n",
    "    if file_type == \"mkv\":\n",
    "        # Load the input video\n",
    "        input_video = VideoFileClip(segment_file)\n",
    "        input_duration = input_video.duration\n",
    "        audio_clip = input_video.audio\n",
    "    elif file_type == \"wav\":\n",
    "        audio_clip = AudioFileClip(segment_file)\n",
    "        input_duration = audio_clip.duration\n",
    "\n",
    "    # Get the duration of the input video\n",
    "\n",
    "    # Create a color clip with the given frame size, color, and duration\n",
    "    background_clip = ColorClip(size=frame_size, color=(0, 0, 0)).set_duration(\n",
    "        input_duration\n",
    "    )\n",
    "\n",
    "    final_video = CompositeVideoClip([background_clip] + all_linelevel_splits)\n",
    "\n",
    "    # Set the audio of the final video to be the same as the input video\n",
    "    final_video = final_video.set_audio(audio_clip)\n",
    "\n",
    "    # Save the final clip as a video file with the audio included\n",
    "    final_video.write_videofile(\n",
    "        out_file,\n",
    "        fps=24,\n",
    "        codec=\"libx264\",\n",
    "        audio_codec=\"aac\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
